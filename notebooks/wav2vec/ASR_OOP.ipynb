{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from datasets import load_dataset\n",
    "from transformers import (Wav2Vec2ForCTC, Wav2Vec2Processor, \n",
    "                          TrainingArguments, Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "                       \n",
    "class Wav2VecTrainer:\n",
    "    def __init__(self, vocab_path, model_name=\"facebook/wav2vec2-large-xlsr-53\"):\n",
    "        # Load tokenizer and feature extractor (processor)\n",
    "        self.tokenizer = Wav2Vec2CTCTokenizer(vocab_path, unk_token=\"<unk>\", pad_token=\"<pad>\", word_delimiter_token=\"|\")\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n",
    "        self.processor = Wav2Vec2Processor(feature_extractor=self.feature_extractor, tokenizer=self.tokenizer)\n",
    "        \n",
    "        # Load pre-trained wav2vec2 model\n",
    "        self.model = Wav2Vec2ForCTC.from_pretrained(\n",
    "            model_name,\n",
    "            attention_dropout=0.1,\n",
    "            hidden_dropout=0.1,\n",
    "            feat_proj_dropout=0.0,\n",
    "            mask_time_prob=0,\n",
    "            layerdrop=0.1,\n",
    "            ctc_loss_reduction=\"mean\",\n",
    "            pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "            vocab_size=len(self.processor.tokenizer),\n",
    "        )\n",
    "\n",
    "    def load_data(self, dataset_name, split=\"train\"):\n",
    "        # Load dataset\n",
    "        self.dataset = load_dataset(dataset_name, split=split)\n",
    "        self.dataset = self.dataset.map(self._prepare_example, remove_columns=self.dataset.column_names)\n",
    "\n",
    "    def _prepare_example(self, batch):\n",
    "        # Load and process audio file\n",
    "        audio = batch[\"audio\"]\n",
    "        batch[\"input_values\"] = self.processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "        # Encode target labels\n",
    "        batch[\"labels\"] = self.processor.tokenizer(batch[\"transcript\"]).input_ids\n",
    "        return batch\n",
    "\n",
    "    def train(self, output_dir=\"./wav2vec2-output\", batch_size=8, epochs=3):\n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            group_by_length=True,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            gradient_accumulation_steps=2,\n",
    "            evaluation_strategy=\"steps\",\n",
    "            num_train_epochs=epochs,\n",
    "            fp16=True,\n",
    "            save_steps=400,\n",
    "            eval_steps=400,\n",
    "            logging_steps=400,\n",
    "            learning_rate=1e-4,\n",
    "            warmup_steps=500,\n",
    "            save_total_limit=2,\n",
    "        )\n",
    "\n",
    "        # Initialize Trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            data_collator=self._data_collator,\n",
    "            args=training_args,\n",
    "            train_dataset=self.dataset,\n",
    "            tokenizer=self.processor.feature_extractor,\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "\n",
    "    def _data_collator(self, features):\n",
    "        # Collate data into batches\n",
    "        input_values = [feature[\"input_values\"] for feature in features]\n",
    "        labels = [feature[\"labels\"] for feature in features]\n",
    "        \n",
    "        # Zero-pad inputs and labels\n",
    "        batch = self.processor.pad({\"input_values\": input_values, \"labels\": labels}, return_tensors=\"pt\")\n",
    "        \n",
    "        # Replace padding with -100 to ignore during CTC loss calculation\n",
    "        batch[\"labels\"] = torch.where(batch[\"labels\"] == self.processor.tokenizer.pad_token_id, -100, batch[\"labels\"])\n",
    "        return batch\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    trainer = Wav2VecTrainer(vocab_path=\"./vocab.json\")\n",
    "    trainer.load_data(dataset_name=\"common_voice\", split=\"train\")\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
