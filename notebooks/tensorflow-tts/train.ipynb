{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Trainer: Where the âœ¨ï¸ happens.\n",
    "# TrainingArgs: Defines the set of arguments of the Trainer.\n",
    "from trainer import Trainer, TrainerArgs\n",
    "\n",
    "# GlowTTSConfig: all model related values for training, validating and testing.\n",
    "from TTS.tts.configs.fast_speech_config import FastSpeechConfig\n",
    "\n",
    "# BaseDatasetConfig: defines name, formatter and path of the dataset.\n",
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.models.forward_tts import ForwardTTS\n",
    "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
    "from TTS.utils.audio import AudioProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.getcwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = BaseDatasetConfig(\n",
    "    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"LJSpeech-1.1/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE THE TRAINING CONFIGURATION\n",
    "# Configure the model. Every config class inherits the BaseTTSConfig.\n",
    "# config = ForwardTTSArgs()\n",
    "config = FastSpeechConfig(\n",
    "    batch_size=8,\n",
    "    eval_batch_size=2,\n",
    "    num_loader_workers=4,\n",
    "    num_eval_loader_workers=4,\n",
    "    run_eval=True,\n",
    "    test_delay_epochs=-1,\n",
    "    epochs=100,\n",
    "    text_cleaner=\"phoneme_cleaners\",\n",
    "    use_phonemes=True,\n",
    "    phoneme_language=\"en-us\",\n",
    "    phoneme_cache_path=\"phoneme_cache\",\n",
    "    print_step=10,\n",
    "    print_eval=True,\n",
    "    mixed_precision=True,\n",
    "    output_path=\"output\",\n",
    "    datasets=[dataset_config],\n",
    "    grad_clip=1,\n",
    "    log_model_step=20,\n",
    "    plot_step=10,\n",
    "    use_noise_augment=True,\n",
    "    lr=0.00001,\n",
    "    lr_scheduler_params={\"warmup_steps\": 300},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:True\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:None\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:45\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n"
     ]
    }
   ],
   "source": [
    "ap = AudioProcessor.init_from_config(config)\n",
    "\n",
    "# INITIALIZE THE TOKENIZER\n",
    "# Tokenizer is used to convert text to sequences of token IDs.\n",
    "# If characters are not defined in the config, default characters are passed to the config\n",
    "tokenizer, config = TTSTokenizer.init_from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap.resample = True\n",
    "ap.do_sound_norm = True\n",
    "# ap.do_rms_norm = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.eval_split_size = 0.012195121951219513"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Found 619 files in /mnt/45b9faff-45f3-43f2-903f-9b92a9a6338c/major-project/notebooks/tensorflow-tts/LJSpeech-1.1\n"
     ]
    }
   ],
   "source": [
    "train_samples, eval_samples = load_tts_samples(\n",
    "    dataset_config,\n",
    "    eval_split=True,\n",
    "    eval_split_max_size=config.eval_split_max_size,\n",
    "    eval_split_size=config.eval_split_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ForwardTTS(config, ap, tokenizer, speaker_manager=None)\n",
    "# model = ForwardTTS(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " > Training Environment:\n",
      " | > Backend: Torch\n",
      " | > Mixed precision: True\n",
      " | > Precision: fp16\n",
      " | > Current device: 0\n",
      " | > Num. of GPUs: 1\n",
      " | > Num. of CPUs: 8\n",
      " | > Num. of Torch Threads: 4\n",
      " | > Torch seed: 54321\n",
      " | > Torch CUDNN: True\n",
      " | > Torch CUDNN deterministic: False\n",
      " | > Torch CUDNN benchmark: False\n",
      " | > Torch TF32 MatMul: False\n",
      " > Start Tensorboard: tensorboard --logdir=output/run-January-01-2025_08+26PM-252392a\n",
      "/mnt/45b9faff-45f3-43f2-903f-9b92a9a6338c/major-project/env/lib/python3.10/site-packages/trainer/trainer.py:552: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler()\n",
      "\n",
      " > Model has 37022561 parameters\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    TrainerArgs(), config, \"output\", model=model, train_samples=train_samples, eval_samples=eval_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 0/100\u001b[0m\n",
      " --> output/run-January-01-2025_08+26PM-252392a\n",
      "\n",
      "\u001b[1m > TRAINING (2025-01-01 20:26:54) \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "> DataLoader initialization\n",
      "| > Tokenizer:\n",
      "\t| > add_blank: False\n",
      "\t| > use_eos_bos: False\n",
      "\t| > use_phonemes: True\n",
      "\t| > phonemizer:\n",
      "\t\t| > phoneme language: en-us\n",
      "\t\t| > phoneme backend: gruut\n",
      "| > Number of instances : 613\n",
      " | > Preprocessing samples\n",
      " | > Max text length: 169\n",
      " | > Min text length: 21\n",
      " | > Avg text length: 79.71778140293638\n",
      " | \n",
      " | > Max audio length: 1244201.0\n",
      " | > Min audio length: 95271.0\n",
      " | > Avg audio length: 387105.7243066884\n",
      " | > Num. instances discarded samples: 0\n",
      " | > Batch group size: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/45b9faff-45f3-43f2-903f-9b92a9a6338c/major-project/env/lib/python3.10/site-packages/TTS/tts/models/forward_tts.py:743: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=False):\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-01-01 20:27:07 -- STEP: 0/307 -- GLOBAL_STEP: 0\u001b[0m\n",
      "     | > loss_spec: 2.6727843284606934  (2.6727843284606934)\n",
      "     | > loss_dur: 1.190316915512085  (1.190316915512085)\n",
      "     | > loss_aligner: 17.09288787841797  (17.09288787841797)\n",
      "     | > loss_binary_alignment: 3.511650323867798  (3.511650323867798)\n",
      "     | > loss: 24.467639923095703  (24.467639923095703)\n",
      "     | > duration_error: 5.6094279289245605  (5.6094279289245605)\n",
      "     | > amp_scaler: 32768.0  (32768.0)\n",
      "     | > grad_norm: 0  (0)\n",
      "     | > current_lr: 3.333333333333334e-08 \n",
      "     | > step_time: 8.5044  (8.504414796829224)\n",
      "     | > loader_time: 4.2978  (4.2977516651153564)\n",
      "\n",
      "/mnt/45b9faff-45f3-43f2-903f-9b92a9a6338c/major-project/env/lib/python3.10/site-packages/TTS/tts/models/forward_tts.py:743: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=False):\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-01-01 20:27:16 -- STEP: 10/307 -- GLOBAL_STEP: 10\u001b[0m\n",
      "     | > loss_spec: 3.7792012691497803  (3.008633279800415)\n",
      "     | > loss_dur: 1.5952441692352295  (1.5044141173362733)\n",
      "     | > loss_aligner: 26.886241912841797  (21.139652442932128)\n",
      "     | > loss_binary_alignment: 3.222242593765259  (3.4503798961639403)\n",
      "     | > loss: 35.482933044433594  (29.103080368041994)\n",
      "     | > duration_error: 9.194417953491211  (6.876115226745606)\n",
      "     | > amp_scaler: 32768.0  (32768.0)\n",
      "     | > grad_norm: tensor(36.9878, device='cuda:0')  (tensor(25.2322, device='cuda:0'))\n",
      "     | > current_lr: 3.333333333333334e-08 \n",
      "     | > step_time: 0.8154  (0.6543628454208374)\n",
      "     | > loader_time: 0.0019  (0.002406024932861328)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-01-01 20:27:22 -- STEP: 20/307 -- GLOBAL_STEP: 20\u001b[0m\n",
      "     | > loss_spec: 2.690685510635376  (3.011411166191101)\n",
      "     | > loss_dur: 1.4195644855499268  (1.463367086648941)\n",
      "     | > loss_aligner: 19.914196014404297  (20.7679123878479)\n",
      "     | > loss_binary_alignment: 3.707019329071045  (3.561540460586548)\n",
      "     | > loss: 27.73146629333496  (28.804231548309325)\n",
      "     | > duration_error: 6.061487197875977  (6.576796078681946)\n",
      "     | > amp_scaler: 32768.0  (32768.0)\n",
      "     | > grad_norm: tensor(21.5834, device='cuda:0')  (tensor(25.8757, device='cuda:0'))\n",
      "     | > current_lr: 3.333333333333334e-08 \n",
      "     | > step_time: 0.4581  (0.6045542597770691)\n",
      "     | > loader_time: 0.0029  (0.0022919416427612306)\n",
      "\n",
      " > Keyboard interrupt detected.\n",
      " > Saving model before exiting...\n",
      "\n",
      " > CHECKPOINT : output/run-January-01-2025_08+26PM-252392a/checkpoint_21.pth\n"
     ]
    }
   ],
   "source": [
    "# AND... 3,2,1... ðŸš€\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "\n",
    "\n",
    "def check_audio_files(directory):\n",
    "    corrupted_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith((\".wav\")):  # Adjust extensions as per your dataset\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    audio, sr = librosa.load(file_path, sr=None)\n",
    "                    if len(audio) == 0:\n",
    "                        raise ValueError(\"Empty audio file\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Corrupted: {file_path}, Error: {e}\")\n",
    "                    corrupted_files.append(file_path)\n",
    "    return corrupted_files\n",
    "\n",
    "\n",
    "# Replace with the path to your dataset\n",
    "dataset_directory = (\n",
    "    \"/mnt/45b9faff-45f3-43f2-903f-9b92a9a6338c/test/tensorflow-tts/LJSpeech-1.1/wavs\"\n",
    ")\n",
    "corrupted_audio_files = check_audio_files(dataset_directory)\n",
    "print(f\"Found {len(corrupted_audio_files)} corrupted files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "\n",
    "def check_audio_duration(directory, min_duration=0.5):  # Minimum duration in seconds\n",
    "    corrupted_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith((\".wav\", \".mp3\")):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    waveform, sr = torchaudio.load(file_path)\n",
    "                    duration = waveform.size(1) / sr\n",
    "                    if duration < min_duration:\n",
    "                        print(f\"Short audio file: {file_path}, Duration: {duration}s\")\n",
    "                        corrupted_files.append(file_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"Corrupted: {file_path}, Error: {e}\")\n",
    "                    corrupted_files.append(file_path)\n",
    "    return corrupted_files\n",
    "\n",
    "\n",
    "dataset_directory = (\n",
    "    \"/mnt/45b9faff-45f3-43f2-903f-9b92a9a6338c/test/tensorflow-tts/LJSpeech-1.1/wavs\"\n",
    ")\n",
    "short_or_corrupted_files = check_audio_duration(dataset_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
