{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tacotron2.model import Tacotron2\n",
    "from tacotron2.hparams import create_hparams\n",
    "from tacotron2.loss_function import Tacotron2Loss\n",
    "from tacotron2.data_utils import TextMelCollate, TextMelLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_pad(text_list, tokenizer, pad_token=0):\n",
    "    tokenized = [torch.tensor(tokenizer(text), dtype=torch.long) for text in text_list]\n",
    "    padded = pad_sequence(tokenized, batch_first=True, padding_value=pad_token)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy tokenizer function (replace with actual tokenizer as needed)\n",
    "def dummy_tokenizer(text):\n",
    "    # text_ecoded = TextMelLoader(text)\n",
    "    return [ord(char) for char in text] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract prosodic features from Nepali raw audio\n",
    "def extract_prosody_features(audio_path):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    pitch = librosa.pyin(y, fmin=50, fmax=300, sr=sr)[0]  # F0\n",
    "    energy = librosa.feature.rms(y=y)[0]  # Root mean square energy\n",
    "    duration = len(y) / sr  # Duration in seconds\n",
    "    return {\n",
    "        \"pitch\": pitch / np.max(pitch),\n",
    "        \"energy\": energy / np.max(energy),\n",
    "        \"duration\": duration,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset to include prosody embeddings from Nepali audio\n",
    "class ProsodyTextMelDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract Nepali audio path, English text, and English audio path from CSV\n",
    "        nepali_audio_path = self.data.iloc[idx][\"nepali_audio\"]\n",
    "        english_text = self.data.iloc[idx][\"english_text\"]\n",
    "        english_audio_path = self.data.iloc[idx][\"english_audio\"]\n",
    "\n",
    "        # Load mel spectrogram for English audio\n",
    "        y_english, sr_english = librosa.load(english_audio_path, sr=None)\n",
    "        mel = librosa.feature.melspectrogram(y=y_english, sr=sr_english)\n",
    "        mel = torch.tensor(mel, dtype=torch.float32)\n",
    "\n",
    "        # Extract prosodic features from Nepali audio\n",
    "        prosody_features = extract_prosody_features(nepali_audio_path)\n",
    "        prosody_embedding = torch.tensor([\n",
    "            np.mean(prosody_features[\"pitch\"]),\n",
    "            np.mean(prosody_features[\"energy\"]),\n",
    "            prosody_features[\"duration\"]\n",
    "        ], dtype=torch.float32)\n",
    "\n",
    "        return english_text, mel, prosody_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified Tacotron2 model to accept prosody embeddings\n",
    "class Tacotron2WithProsody(Tacotron2):\n",
    "    def __init__(self, hparams):\n",
    "        super(Tacotron2WithProsody, self).__init__(hparams)\n",
    "        # Adding a prosody conditioning layer\n",
    "        self.prosody_embedding_layer = nn.Linear(3, hparams.decoder_rnn_dim)\n",
    "\n",
    "    def parse_batch(self, batch):\n",
    "        text_padded, input_lengths, mel_padded, gate_padded, output_lengths, prosody = batch\n",
    "        return (\n",
    "            (text_padded, input_lengths, mel_padded, output_lengths, prosody),\n",
    "            (mel_padded, gate_padded)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        text_inputs, text_lengths, mels, output_lengths, prosody = inputs\n",
    "\n",
    "        # Original Tacotron2 forward pass\n",
    "        embedded_inputs = self.embedding(text_inputs).transpose(1, 2)\n",
    "        encoder_outputs = self.encoder(embedded_inputs, text_lengths)\n",
    "\n",
    "        # Condition decoder on prosody embeddings\n",
    "        prosody_embedding = self.prosody_embedding_layer(prosody)\n",
    "        decoder_inputs = self.decoder.prenet(mels[:, :-1, :])\n",
    "        decoder_inputs = decoder_inputs + prosody_embedding.unsqueeze(1)\n",
    "\n",
    "        mel_outputs, gate_outputs, alignments = self.decoder(\n",
    "            decoder_inputs, encoder_outputs, memory_lengths=text_lengths\n",
    "        )\n",
    "        mel_outputs_postnet = self.postnet(mel_outputs) + mel_outputs\n",
    "\n",
    "        return mel_outputs, mel_outputs_postnet, gate_outputs, alignments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tacotron2_with_prosody(model, dataset, hparams, checkpoint_path, epochs=50):\n",
    "    # Initialize DataLoader and optimizer\n",
    "    collate_fn = TextMelCollate(hparams.n_frames_per_step)\n",
    "    dataloader = DataLoader(dataset, batch_size=hparams.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=hparams.learning_rate)\n",
    "    criterion = Tacotron2Loss()\n",
    "\n",
    "    # Move model to device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch in dataloader:\n",
    "            # Parse batch and move to device\n",
    "            inputs, targets = model.parse_batch(batch)\n",
    "            inputs = tuple(i.to(device) for i in inputs)\n",
    "            targets = tuple(t.to(device) for t in targets)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            mel_outputs, mel_outputs_postnet, gate_outputs, _ = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion((mel_outputs, mel_outputs_postnet, gate_outputs), targets)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(dataloader)}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save({'state_dict': model.state_dict()}, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_hparams():\n",
    "#     hparams = {\n",
    "#         'batch_size': 16,\n",
    "#         'n_frames_per_step': 1,\n",
    "#         'learning_rate': 1e-3,\n",
    "#         'decoder_rnn_dim': 1024,\n",
    "#         'max_wav_value': 32768.0,\n",
    "#         'sampling_rate': 22050,\n",
    "#         'mel_fmin': 0.0,\n",
    "#         'mel_fmax': 8000.0,\n",
    "#     }\n",
    "#     return hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m Tacotron2WithProsody(hparams)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtrain_tacotron2_with_prosody\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtacotron2_with_prosody_checkpoint.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[48], line 15\u001b[0m, in \u001b[0;36mtrain_tacotron2_with_prosody\u001b[0;34m(model, dataset, hparams, checkpoint_path, epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     14\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Parse batch and move to device\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mparse_batch(batch)\n\u001b[1;32m     18\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(i\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m inputs)\n",
      "File \u001b[0;32m/mnt/45b9faff-45f3-43f2-903f-9b92a9a6338c/major-project/notebooks/tacotron/env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/mnt/45b9faff-45f3-43f2-903f-9b92a9a6338c/major-project/notebooks/tacotron/env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/mnt/45b9faff-45f3-43f2-903f-9b92a9a6338c/major-project/notebooks/tacotron/env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/45b9faff-45f3-43f2-903f-9b92a9a6338c/major-project/notebooks/tacotron/env/lib/python3.10/site-packages/tacotron2/data_utils.py:89\u001b[0m, in \u001b[0;36mTextMelCollate.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ids_sorted_decreasing)):\n\u001b[1;32m     88\u001b[0m     text \u001b[38;5;241m=\u001b[39m batch[ids_sorted_decreasing[i]][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 89\u001b[0m     text_padded[i, :\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m(\u001b[38;5;241m0\u001b[39m)] \u001b[38;5;241m=\u001b[39m text\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Right zero-pad mel-spec\u001b[39;00m\n\u001b[1;32m     92\u001b[0m num_mels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    hparams = create_hparams()\n",
    "\n",
    "    # Load dataset from CSV\n",
    "    csv_file = \"dataset.csv\"  # CSV file containing Nepali audio, English text, and English audio paths\n",
    "    dataset = ProsodyTextMelDataset(csv_file)\n",
    "\n",
    "    # Initialize model\n",
    "    model = Tacotron2WithProsody(hparams)\n",
    "\n",
    "    # Train model\n",
    "    train_tacotron2_with_prosody(model, dataset, hparams, \"tacotron2_with_prosody_checkpoint.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['english_audio'] = df['english_audio'].apply(lambda x: \"./audio/\" + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nepali_audio'] = df['nepali_audio'].apply(lambda x: \"./audio/\" + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nepali_audio</th>\n",
       "      <th>english_text</th>\n",
       "      <th>english_audio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./audio/PRB_Nep_01_Bhojpur_01m4a</td>\n",
       "      <td>Hello friends Today we are embarking on a jour...</td>\n",
       "      <td>./audio/PRB_Eng_01_Bhojpur_01m4a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./audio/PRB_Nep_01_Bhojpur_02m4a</td>\n",
       "      <td>This district is located in Province No 1 of N...</td>\n",
       "      <td>./audio/PRB_Eng_01_Bhojpur_02m4a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./audio/PRB_Nep_01_Bhojpur_03m4a</td>\n",
       "      <td>The natural beauty of Bhojpur is captivating u...</td>\n",
       "      <td>./audio/PRB_Eng_01_Bhojpur_03m4a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./audio/PRB_Nep_01_Bhojpur_04m4a</td>\n",
       "      <td>We plan to visit some of the famous temples here</td>\n",
       "      <td>./audio/PRB_Eng_01_Bhojpur_04m4a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./audio/PRB_Nep_01_Bhojpur_05m4a</td>\n",
       "      <td>Our first stop an ancient temple in Bhojpur Th...</td>\n",
       "      <td>./audio/PRB_Eng_01_Bhojpur_05m4a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       nepali_audio  \\\n",
       "0  ./audio/PRB_Nep_01_Bhojpur_01m4a   \n",
       "1  ./audio/PRB_Nep_01_Bhojpur_02m4a   \n",
       "2  ./audio/PRB_Nep_01_Bhojpur_03m4a   \n",
       "3  ./audio/PRB_Nep_01_Bhojpur_04m4a   \n",
       "4  ./audio/PRB_Nep_01_Bhojpur_05m4a   \n",
       "\n",
       "                                        english_text  \\\n",
       "0  Hello friends Today we are embarking on a jour...   \n",
       "1  This district is located in Province No 1 of N...   \n",
       "2  The natural beauty of Bhojpur is captivating u...   \n",
       "3   We plan to visit some of the famous temples here   \n",
       "4  Our first stop an ancient temple in Bhojpur Th...   \n",
       "\n",
       "                      english_audio  \n",
       "0  ./audio/PRB_Eng_01_Bhojpur_01m4a  \n",
       "1  ./audio/PRB_Eng_01_Bhojpur_02m4a  \n",
       "2  ./audio/PRB_Eng_01_Bhojpur_03m4a  \n",
       "3  ./audio/PRB_Eng_01_Bhojpur_04m4a  \n",
       "4  ./audio/PRB_Eng_01_Bhojpur_05m4a  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
